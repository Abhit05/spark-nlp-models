{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os, zipfile, json, pandas as pd, shutil, boto3, datetime, tabulate, pytz\n",
    "from collections import OrderedDict\n",
    "\n",
    "sys.path.append(\"/home/fernandrez/JSL/repos/spark-nlp/python\")\n",
    "sys.path.append(\"/home/fernandrez/JSL/repos/spark-nlp-internal/python\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = boto3.client('s3')\n",
    "bucket = 'auxdata.johnsnowlabs.com'\n",
    "bucket_url = f\"https://s3.console.aws.amazon.com/s3/object/{bucket}\"\n",
    "download_url = f\"s3://{bucket}/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_s3_metadata(prefix=\"clinical/models\"):\n",
    "    return s3.list_objects_v2(\n",
    "    Bucket=bucket,\n",
    "    EncodingType='url',\n",
    "    Prefix=prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_desired_names(content):\n",
    "    return [k[\"Key\"] for k in content\n",
    "         if k[\"Key\"][-3:]==\"zip\" and \n",
    "         \"_en_2\" in k[\"Key\"] and \n",
    "         \"2ng\" not in k[\"Key\"]and \n",
    "         \"icdoem\" not in k[\"Key\"] and\n",
    "         \"snomed_l\" not in k[\"Key\"] and\n",
    "         \"rxnorm_l\" not in k[\"Key\"] and\n",
    "         \"/resolve\" not in k[\"Key\"] and\n",
    "         \"noncontrib\" not in k[\"Key\"] and\n",
    "         \"embeddings_icd10_base\" not in k[\"Key\"] and\n",
    "         \"icdem\" not in k[\"Key\"] and\n",
    "         \"demo\" not in k[\"Key\"] and\n",
    "         \"_n2c\" not in k[\"Key\"] and\n",
    "         \"people_disambiguator\" not in k[\"Key\"]\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_latest(model_list, lang=\"en\"):\n",
    "    model_tuples = [(*m.split(f\"_{lang}_\"),m) for m in model_list if lang in m]\n",
    "    model_tuples_versions = [(*m[0].rsplit(\"/\",1),*(m[1].split(\"_\")),m[2]) for m in model_tuples]\n",
    "    model_names = [(m[0],m[1],lang,f\"`{m[2]}`\",f\"`{m[3]}`\",m[4][:-4],\n",
    "                    f\"[:floppy_disk:]({bucket_url}/{m[5]} 'Download')\",\n",
    "                    f\"[:computer:]({download_url}/{m[5]} 'S3')\") \n",
    "                   for m in model_tuples_versions]\n",
    "    all_models = pd.DataFrame(model_names, \n",
    "                      columns=[\"Collection\",\"Name\",\"Lang\",\"Build\",\"Spark Version\",\"Timestamp\",\"Download\",\"S3\"])\n",
    "    latest = pd.DataFrame(all_models.groupby(\"Name\")[\"Timestamp\"].max()).reset_index()\\\n",
    "                        .set_index([\"Name\",\"Timestamp\"])\n",
    "    names = all_models.set_index([\"Name\",\"Timestamp\"]).join(latest, how=\"inner\")\n",
    "    names.reset_index(inplace=True)\n",
    "    names[\"Name\"] = names[\"Name\"].str.split(\"/\").apply(lambda x: x[-1])\n",
    "    names[\"Date\"] = names[\"Timestamp\"].apply(lambda x: datetime.datetime.fromtimestamp(int(x)/1000).strftime(\"%Y-%m-%d\"))\n",
    "    names.reset_index(inplace=True)\n",
    "    return names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_github_table(outpath=\"/home/fernandrez/JSL/repos/spark-nlp-models/enterprise.md\"):\n",
    "    metadata = pd.read_csv(\"/home/fernandrez/JSL/repos/spark-nlp-models/entrerprise/model_metadata.csv\")\n",
    "    names = filter_desired_names(get_s3_metadata()[\"Contents\"])\n",
    "    model_list = get_latest(names).set_index(\"Name\").join(metadata.set_index(\"Name\"), how=\"inner\")\\\n",
    "    .reset_index().fillna(\"\")\n",
    "    model_list[\"Name\"] = model_list[\"Name\"].apply(lambda x: f\"`{x}`\")\n",
    "    model_list[\"Model\"] = model_list[\"Model\"].apply(lambda x: f\"`{x}`\")\n",
    "    model_list[\"Build\"] = model_list[\"Build\"].apply(lambda x: f\"{x}\")\n",
    "    model_list[\"TrainedOn\"] = model_list[[\"TrainedOn\", \"DatasetLink\"]].apply(\n",
    "        lambda x: f\"[:clipboard:]({(x.DatasetLink if x.DatasetLink else '#')} '{x.TrainedOn}')\", axis=1)\n",
    "    model_list[\"Extracts\"] = model_list[\"Extracts\"].apply(lambda x: f\"[:mag:](# 'Extracts: {x}')\" if x else \"\")\n",
    "    model_list.drop(\"DatasetLink\", axis=1, inplace=True)\n",
    "    model_list.sort_values(\"Model\", inplace=True)\n",
    "    selhdrs=[\"Model\", \"Name\", \"Build\", \"Extracts\", \"TrainedOn\", \"Download\"]\n",
    "    table = tabulate.tabulate(model_list[selhdrs],\n",
    "                              headers = selhdrs[:-3]+[\"\",\"\",\"\"],\n",
    "                              tablefmt=\"github\",\n",
    "                             showindex=\"false\")\n",
    "    with open(outpath,\"w\") as f:\n",
    "        f.write(table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_aux_dict(prefix, model_prefix):\n",
    "    response = get_s3_metadata(prefix)\n",
    "    if prefix[-1] != \"/\":\n",
    "        prefix = prefix+\"/\"\n",
    "    print(\"{\")\n",
    "    for d in response[\"Contents\"]:\n",
    "        print('\"'+d[\"Key\"].replace(prefix,\"\")+f'\":\"{model_prefix}_\",')\n",
    "    print(\"}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## WRITE TABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_github_table()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PUBLISH MODELS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "\"JNLPBA_NER_model_20200420_30e_b32.zip\":\"ner_\",\n",
      "\"anatomy_NER_model_20200314_30e_b8.zip\":\"ner_\",\n",
      "\"deid_NER_model_20200326_7_labels_20e_b8.zip\":\"ner_\",\n",
      "\"deid_NER_model_20200326_enriched_labels_50e_b8.zip\":\"ner_\",\n",
      "\"deid_NER_model_large_20200406_7_labels_30e_b32.zip\":\"ner_\",\n",
      "\"disease%2Bi2b2_NER_model_20200306.zip\":\"ner_\",\n",
      "\"drug_NER_model_20200306.zip\":\"ner_\",\n",
      "\"jsl_internal_NER_model_20200325_50e_b16.zip\":\"ner_\",\n",
      "\"jsl_internal_NER_model_20200325_52_labels_30e_b8.zip\":\"ner_\",\n",
      "\"posology_NER_2018_large_model_v2_20200403_7_labels_30e_b32.zip\":\"ner_\",\n",
      "\"posology_NER_2018_v2_model_20200402_7_labels_30e_b32.zip\":\"ner_\",\n",
      "\"posology_NER_model_large_20200309_10e.zip\":\"ner_\",\n",
      "\"risk_factor_NER_model_20200401_19_labels_50e_b8.zip\":\"ner_\",\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "source_prefix = \"clinical/resources/temp_models\"\n",
    "model_prefix = \"ner\"\n",
    "print_aux_dict(prefix, model_prefix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = {\n",
    "\"JNLPBA_NER_model_20200420_30e_b32.zip\":\"ner_cellular\",\n",
    "\"anatomy_NER_model_20200314_30e_b8.zip\":\"ner_anatomy\",\n",
    "\"deid_NER_model_20200326_enriched_labels_50e_b8.zip\":\"ner_deid_enriched\",\n",
    "\"deid_NER_model_large_20200406_7_labels_30e_b32.zip\":\"ner_deid_large\",\n",
    "\"jsl_internal_NER_model_20200325_50e_b16.zip\":\"ner_jsl\",\n",
    "\"jsl_internal_NER_model_20200325_52_labels_30e_b8.zip\":\"ner_jsl_enriched\",\n",
    "\"posology_NER_2018_large_model_v2_20200403_7_labels_30e_b32.zip\":\"ner_posology_large\",\n",
    "\"posology_NER_2018_v2_model_20200402_7_labels_30e_b32.zip\":\"ner_posology_small\",\n",
    "\"risk_factor_NER_model_20200401_19_labels_50e_b8.zip\":\"ner_risk_factors\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def publish_models(names, lang, libv, sparkv, correctly_zipped=True):\n",
    "    metas=[]\n",
    "    ts = int(datetime.datetime.timestamp(datetime.datetime.now())*1000)\n",
    "    libparts = '{{\"parts\":[{}]}}'.format(\",\".join(libv.split(\".\")))\n",
    "    sparkparts = '{{\"parts\":[{}]}}'.format(\",\".join(sparkv.split(\".\")))\n",
    "    for p, n in names.items():\n",
    "        ts -= 1000\n",
    "        tme = datetime.datetime.fromtimestamp(ts/1000,pytz.utc).strftime(\"%Y-%m-%dT%H:%M:%S.%f\")[:-3]+\"Z\"\n",
    "        nm = f'{{\"name\":\"{n}\",\"language\":\"{lang}\",\"libVersion\":{libparts},\"sparkVersion\":{sparkparts},\"readyToUse\":true,\"time\":\"{tme}\",\"isZipped\":true,\"category\":\"nd\",\"checksum\":\"\"}}'\n",
    "        fn = f\"{n}_{lang}_{libv}_{sparkv}_{ts}\"\n",
    "        metas.append(nm)\n",
    "        if correctly_zipped:\n",
    "            s3.copy({\"Bucket\":bucket,\"Key\":os.path.join(prefix,p)}, \n",
    "                    bucket, os.path.join(\"clinical/models\", fn+\".zip\"))\n",
    "        else:\n",
    "            s3.download_file(bucket, os.path.join(prefix,p), os.path.join(\"temp_models\",n))\n",
    "            with zipfile.ZipFile(os.path.join(\"temp_models\",n), 'r') as zip_ref:\n",
    "                zip_ref.extractall(os.path.join(\"temp_models\",n+\"_unzipped\"))\n",
    "            os.remove(os.path.join(\"temp_models\",n))\n",
    "            for root, dirs, files in os.walk(os.path.join(\"temp_models\",n+\"_unzipped\"), topdown=True): \n",
    "                if \"metadata\" in dirs:\n",
    "                    break\n",
    "            shutil.make_archive(os.path.join(\"temp_models\",fn), \"zip\", root)\n",
    "            shutil. rmtree(os.path.join(\"temp_models\",n+\"_unzipped\"))\n",
    "            s3.upload_file(os.path.join(\"temp_models\",fn+\".zip\"), bucket,  os.path.join(\"clinical/models\", fn+\".zip\"))\n",
    "            os.remove(os.path.join(\"temp_models\",fn+\".zip\"))\n",
    "    return metas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "libv=\"2.4.2\"\n",
    "sparkv=\"2.4\"\n",
    "lang=\"en\"\n",
    "correctly_zipped = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"name\":\"ner_cellular\",\"language\":\"en\",\"libVersion\":{\"parts\":[2,4,2]},\"sparkVersion\":{\"parts\":[2,4]},\"readyToUse\":true,\"time\":\"2020-04-21T23:55:08.751Z\",\"isZipped\":true,\"category\":\"nd\",\"checksum\":\"\"}\n",
      "{\"name\":\"ner_anatomy\",\"language\":\"en\",\"libVersion\":{\"parts\":[2,4,2]},\"sparkVersion\":{\"parts\":[2,4]},\"readyToUse\":true,\"time\":\"2020-04-21T23:55:07.751Z\",\"isZipped\":true,\"category\":\"nd\",\"checksum\":\"\"}\n",
      "{\"name\":\"ner_deid_enriched\",\"language\":\"en\",\"libVersion\":{\"parts\":[2,4,2]},\"sparkVersion\":{\"parts\":[2,4]},\"readyToUse\":true,\"time\":\"2020-04-21T23:55:06.751Z\",\"isZipped\":true,\"category\":\"nd\",\"checksum\":\"\"}\n",
      "{\"name\":\"ner_deid_large\",\"language\":\"en\",\"libVersion\":{\"parts\":[2,4,2]},\"sparkVersion\":{\"parts\":[2,4]},\"readyToUse\":true,\"time\":\"2020-04-21T23:55:05.751Z\",\"isZipped\":true,\"category\":\"nd\",\"checksum\":\"\"}\n",
      "{\"name\":\"ner_jsl\",\"language\":\"en\",\"libVersion\":{\"parts\":[2,4,2]},\"sparkVersion\":{\"parts\":[2,4]},\"readyToUse\":true,\"time\":\"2020-04-21T23:55:04.751Z\",\"isZipped\":true,\"category\":\"nd\",\"checksum\":\"\"}\n",
      "{\"name\":\"ner_jsl_enriched\",\"language\":\"en\",\"libVersion\":{\"parts\":[2,4,2]},\"sparkVersion\":{\"parts\":[2,4]},\"readyToUse\":true,\"time\":\"2020-04-21T23:55:03.751Z\",\"isZipped\":true,\"category\":\"nd\",\"checksum\":\"\"}\n",
      "{\"name\":\"ner_posology_large\",\"language\":\"en\",\"libVersion\":{\"parts\":[2,4,2]},\"sparkVersion\":{\"parts\":[2,4]},\"readyToUse\":true,\"time\":\"2020-04-21T23:55:02.751Z\",\"isZipped\":true,\"category\":\"nd\",\"checksum\":\"\"}\n",
      "{\"name\":\"ner_posology_small\",\"language\":\"en\",\"libVersion\":{\"parts\":[2,4,2]},\"sparkVersion\":{\"parts\":[2,4]},\"readyToUse\":true,\"time\":\"2020-04-21T23:55:01.751Z\",\"isZipped\":true,\"category\":\"nd\",\"checksum\":\"\"}\n",
      "{\"name\":\"ner_risk_factors\",\"language\":\"en\",\"libVersion\":{\"parts\":[2,4,2]},\"sparkVersion\":{\"parts\":[2,4]},\"readyToUse\":true,\"time\":\"2020-04-21T23:55:00.751Z\",\"isZipped\":true,\"category\":\"nd\",\"checksum\":\"\"}\n"
     ]
    }
   ],
   "source": [
    "# metas = publish_models(names, lang, libv, sparkv, False)\n",
    "# for m in metas:\n",
    "#     print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"ner_cellular\",\"NerDLModel\",\"\"\n",
      "\"ner_anatomy\",\"NerDLModel\",\"\"\n",
      "\"ner_deid_enriched\",\"NerDLModel\",\"\"\n",
      "\"ner_deid_large\",\"NerDLModel\",\"\"\n",
      "\"ner_jsl\",\"NerDLModel\",\"\"\n",
      "\"ner_jsl_enriched\",\"NerDLModel\",\"\"\n",
      "\"ner_posology_large\",\"NerDLModel\",\"\"\n",
      "\"ner_posology_small\",\"NerDLModel\",\"\"\n",
      "\"ner_risk_factors\",\"NerDLModel\",\"\"\n"
     ]
    }
   ],
   "source": [
    "for v in names.values():\n",
    "    print('\"'+v+'\",\"NerDLModel\",\"\",\"\",\"\"')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ner_cellular\n",
      "ner_cellular download started this may take some time.\n",
      "Approximate size to download 13.9 MB\n",
      "[OK!]\n",
      "ner_anatomy\n",
      "ner_anatomy download started this may take some time.\n",
      "Approximate size to download 14 MB\n",
      "[OK!]\n",
      "ner_deid_enriched\n",
      "ner_deid_enriched download started this may take some time.\n",
      "Approximate size to download 14.2 MB\n",
      "[OK!]\n",
      "ner_deid_large\n",
      "ner_deid_large download started this may take some time.\n",
      "Approximate size to download 14 MB\n",
      "[OK!]\n",
      "ner_jsl\n",
      "ner_jsl download started this may take some time.\n",
      "Approximate size to download 14 MB\n",
      "[OK!]\n",
      "ner_jsl_enriched\n",
      "ner_jsl_enriched download started this may take some time.\n",
      "Approximate size to download 14.1 MB\n",
      "[OK!]\n",
      "ner_posology_large\n",
      "ner_posology_large download started this may take some time.\n",
      "Approximate size to download 13.8 MB\n",
      "[OK!]\n",
      "ner_posology_small\n",
      "ner_posology_small download started this may take some time.\n",
      "Approximate size to download 13.9 MB\n",
      "[OK!]\n",
      "ner_risk_factors\n",
      "ner_risk_factors download started this may take some time.\n",
      "Approximate size to download 13.9 MB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "from sparknlp.annotator import *\n",
    "for p,n in names.items():\n",
    "    print(n)\n",
    "    NerDLModel.pretrained(n, \"en\", \"clinical/models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moveParam(json_data, origin, destination):\n",
    "    params = list(json_data[\"paramMap\"].keys())\n",
    "    defParams = list(json_data[\"defaultParamMap\"].keys())\n",
    "    \n",
    "    if origin not in params+defParams:\n",
    "        print(f\"{origin} not found\")\n",
    "    if origin in params:\n",
    "        final_value = json_data[\"paramMap\"][origin] if json_data[\"paramMap\"][origin]!=\"embeddings_icd10cmo\" else \"embeddings_icdoem\"\n",
    "        json_data[\"paramMap\"][destination] = final_value\n",
    "        del json_data[\"paramMap\"][origin]\n",
    "        print(f\"Moved {origin} to {destination}\")\n",
    "    if origin in defParams:\n",
    "        final_value = json_data[\"defaultParamMap\"][origin] if json_data[\"defaultParamMap\"][origin]!=\"embeddings_icd10cmo\" else \"embeddings_icdoem\"\n",
    "        json_data[\"defaultParamMap\"][destination] = final_value\n",
    "        del json_data[\"defaultParamMap\"][\"embeddingsRef\"]  \n",
    "        \n",
    "def deleteParam(json_data, param):\n",
    "    params = list(json_data[\"paramMap\"].keys())\n",
    "    defParams = list(json_data[\"defaultParamMap\"].keys())\n",
    "    if param in params:\n",
    "        del json_data[\"paramMap\"][param]\n",
    "        print(f\"Deleted {param}\")\n",
    "    if param in defParams:\n",
    "        del json_data[\"defaultParamMap\"][param]\n",
    "        \n",
    "def addParam(json_data, param, value):\n",
    "    params = list(json_data[\"paramMap\"].keys())\n",
    "    defParams = list(json_data[\"defaultParamMap\"].keys())\n",
    "    json_data[\"paramMap\"][param] = value\n",
    "    json_data[\"defaultParamMap\"][param] = value\n",
    "    print(f\"Added {param}:{value}\")\n",
    "\n",
    "def update_json(json_data):\n",
    "    json_data[\"timestamp\"] = timestamp\n",
    "    model_class = json_data[\"class\"]\n",
    "    \n",
    "    moveParam(json_data, \"embeddingsRef\", \"storageRef\")\n",
    "    moveParam(json_data, \"includeEmbeddings\", \"includeStorage\")\n",
    "    \n",
    "    if model_class in [\"com.johnsnowlabs.nlp.annotators.ner.dl.NerDLModel\",\n",
    "                       \"com.johnsnowlabs.nlp.annotators.assertion.dl.AssertionDLModel\",\n",
    "                      \"com.johnsnowlabs.nlp.annotators.assertion.logreg.AssertionLogRegModel\"]:\n",
    "        addParam(json_data, \"storageRef\", \"clinical\")\n",
    "        \n",
    "    return json_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "metadata_dict = OrderedDict({\n",
    "    \"name\":\"textmatch_cpt_token\",\n",
    "    \"language\":\"en\",\n",
    "    \"libVersion\":{\"parts\":[2,4,0]},\n",
    "    \"sparkVersion\":{\"parts\":[2,4]},\n",
    "    \"readyToUse\":True,\n",
    "    \"time\":date,\n",
    "    \"isZipped\":True,\n",
    "    \"category\":\"nd\",\n",
    "    \"checksum\":\"\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "deidentify_rb_en_2.0.2_2.4_1559672122511.zip --> deidentify_rb_en_2.4.0_2.4_1580237286004\n",
      "embeddingsRef not found\n",
      "\n",
      "nerdl_tumour_demo_en_2.0.2_2.4_1558466102322.zip --> nerdl_tumour_demo_en_2.4.0_2.4_1580237286004\n",
      "embeddingsRef not found\n",
      "Added storageRef:clinical\n",
      "\n",
      "assertion_dl_en_2.3.4_2.4_1574888344402.zip --> assertion_dl_en_2.4.0_2.4_1580237286004\n",
      "embeddingsRef not found\n",
      "Added storageRef:clinical\n",
      "\n",
      "chunkresolve_icdo_icdoem_en_2.3.4_2.4_1574890700988.zip --> chunkresolve_icdo_icdoem_en_2.4.0_2.4_1580237286004\n",
      "Moved embeddingsRef to storageRef\n",
      "Deleted includeEmbeddings\n",
      "\n",
      "people_disambiguator_en_2.3.4_2.4_1574806205059.zip --> people_disambiguator_en_2.4.0_2.4_1580237286004\n",
      "embeddingsRef not found\n",
      "\n",
      "deidentify_dl_en_2.0.2_2.4_1559669094458.zip --> deidentify_dl_en_2.4.0_2.4_1580237286004\n",
      "embeddingsRef not found\n",
      "Added storageRef:clinical\n",
      "\n",
      "ner_bionlp_en_2.3.4_2.4_1574889731300.zip --> ner_bionlp_en_2.4.0_2.4_1580237286004\n",
      "embeddingsRef not found\n",
      "Added storageRef:clinical\n",
      "\n",
      "context_spell_med_en_2.0.2_2.4_1564584130634.zip --> context_spell_med_en_2.4.0_2.4_1580237286004\n",
      "embeddingsRef not found\n",
      "\n",
      "ner_clinical_en_2.0.2_2.4_1556659769638.zip --> ner_clinical_en_2.4.0_2.4_1580237286004\n",
      "embeddingsRef not found\n",
      "Added storageRef:clinical\n",
      "\n",
      "ner_clinical_noncontrib_en_2.3.0_2.4_1573751255434.zip --> ner_clinical_noncontrib_en_2.4.0_2.4_1580237286004\n",
      "embeddingsRef not found\n",
      "Added storageRef:clinical\n",
      "\n",
      "resolve_icd10cm_icdem_en_2.2.0_2.4_1570061740522.zip --> resolve_icd10cm_icdem_en_2.4.0_2.4_1580237286004\n",
      "Moved embeddingsRef to storageRef\n",
      "Deleted includeEmbeddings\n",
      "\n",
      "assertion_ml_en_2.3.4_2.4_1574889008594.zip --> assertion_ml_en_2.4.0_2.4_1580237286004\n",
      "embeddingsRef not found\n",
      "Added storageRef:clinical\n",
      "\n",
      "resolve_icd10_en_2.0.2_2.4_1558455679383.zip --> resolve_icd10_en_2.4.0_2.4_1580237286004\n",
      "Moved embeddingsRef to storageRef\n",
      "Deleted includeEmbeddings\n",
      "\n",
      "chunkresolve_cpt_icdoem_en_2.3.4_2.4_1574890805637.zip --> chunkresolve_cpt_icdoem_en_2.4.0_2.4_1580237286004\n",
      "Moved embeddingsRef to storageRef\n",
      "Deleted includeEmbeddings\n",
      "\n",
      "ner_clinical_icdem_en_2.2.0_2.4_1570061780935.zip --> ner_clinical_icdem_en_2.4.0_2.4_1580237286004\n",
      "embeddingsRef not found\n",
      "Added storageRef:clinical\n",
      "\n",
      "embeddings_icd10_base_en_2.2.0_2.4_1570070359789.zip --> embeddings_icd10_base_en_2.4.0_2.4_1580237286004\n",
      "Moved embeddingsRef to storageRef\n",
      "Deleted includeEmbeddings\n",
      "\n",
      "pos_clinical_en_2.0.2_2.4_1556660550177.zip --> pos_clinical_en_2.4.0_2.4_1580237286004\n",
      "embeddingsRef not found\n",
      "\n",
      "spellcheck_clinical_en_2.0.2_2.4_1558461056197.zip --> spellcheck_clinical_en_2.4.0_2.4_1580237286004\n",
      "embeddingsRef not found\n",
      "\n",
      "spellcheck_dl_en_2.2.2_2.4_1573526353270.zip --> spellcheck_dl_en_2.4.0_2.4_1580237286004\n",
      "embeddingsRef not found\n",
      "\n",
      "nercrf_tumour_demo_en_2.0.2_2.4_1558462993066.zip --> nercrf_tumour_demo_en_2.4.0_2.4_1580237286004\n",
      "embeddingsRef not found\n",
      "\n"
     ]
    }
   ],
   "source": [
    "new_metadata = open(os.path.join(model_folder,\"new_metadata.json\"), \"w\")\n",
    "for m in model_list:\n",
    "    full_path = os.path.join(model_folder, m)\n",
    "    name = m.split(\"_en\")[0]\n",
    "    nm = m[:-27]+\"2.4.0_2.4_1580237286004\"\n",
    "    print(m,\"-->\",nm)\n",
    "    extract_path = os.path.join(model_folder, nm)\n",
    "    with zipfile.ZipFile(full_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_path)\n",
    "    json_original = json.load(open(extract_path+\"/metadata/part-00000\",\"r\"))\n",
    "    new_json = update_json(json_original)\n",
    "    json.dump(new_json, open(extract_path+\"/metadata/part-00000\",\"w\"))\n",
    "    os.remove(extract_path+\"/metadata/.part-00000.crc\")\n",
    "    os.remove(extract_path+\"/metadata/._SUCCESS.crc\")\n",
    "    shutil.make_archive(extract_path, 'zip', extract_path)\n",
    "    metadata_dict[\"name\"] = name\n",
    "    new_metadata.write(json.dumps(metadata_dict).replace(\" \",\"\"))\n",
    "    new_metadata.write(\"\\n\")\n",
    "    print()\n",
    "new_metadata.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_new = \"/home/fernandrez/JSL/notebooks/MetistreamRxNorm/models/resolvers_24\"\n",
    "for f in os.listdir(path_new):\n",
    "    if not f.endswith(\"index\"):\n",
    "        try:\n",
    "            os.remove(os.path.join(path_new,f,\"metadata/.part-00000.crc\"))\n",
    "        except OSError:\n",
    "            pass\n",
    "        try:\n",
    "            os.remove(os.path.join(path_new,f,\"metadata/._SUCCESS.crc\"))\n",
    "        except OSError:\n",
    "            pass\n",
    "        shutil.make_archive(os.path.join(path_new,f), 'zip', os.path.join(path_new,f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "model_folder = \"/home/fernandrez/JSL/model_migration_240/a\"\n",
    "#model_folder = \"/home/fernandrez/JSL/notebooks/MetistreamSNOMED/models/resolvers_24\"\n",
    "model_list = os.listdir(model_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddingsRef not found\n",
      "includeEmbeddings not found\n",
      "Added storageRef:clinical\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for m in model_list:\n",
    "    if not m.endswith(\"index\") and not m.endswith(\"zip\"):\n",
    "        full_path = os.path.join(model_folder, m)\n",
    "        json_original = json.load(open(full_path+\"/metadata/part-00000\",\"r\"))\n",
    "        new_json = update_json(json_original)\n",
    "        json.dump(new_json, open(full_path+\"/metadata/part-00000\",\"w\"))\n",
    "        try:\n",
    "            os.remove(full_path+\"/metadata/.part-00000.crc\")\n",
    "        except:\n",
    "            pass\n",
    "        try:\n",
    "            os.remove(full_path+\"/metadata/._SUCCESS.crc\")\n",
    "        except:\n",
    "            pass\n",
    "        shutil.make_archive(full_path, 'zip', full_path)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jsl368",
   "language": "python",
   "name": "jsl368"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
