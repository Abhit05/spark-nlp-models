---
title: Regex tokenizer
author: John Snow Labs
name: token_rule
date: 2020-07-21 11:33:00 +0800
categories: [Open Source, Model]
tags: [embeddings]
---

## Model name: *token_rule*
## Type: *token*
## Compatibility: *Spark NLP 2.4.0*
## License: *Open Source*
## Spark inputs: *[document]*
## Spark outputs: *[token]*
## Language: *[en]*
## TargetPattern: "\\S+"
## Case sensitive: true


## Description
short description 
## How to use
```python

N/A
```
## Run in colab

<p style="text-align:left"> 


[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/streamlit_notebooks/TEXT_PREPROCESSING.ipynb)


</p>


## Streamlit example
<https://demo.johnsnowlabs.com/public/TEXT_PREPROCESSING/>

## Dataset used for training 
N/A

## Evaluation results
N/A

## Download address
<https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/token_rules_en_2.1.0_2.4_1562934409791.zip>

