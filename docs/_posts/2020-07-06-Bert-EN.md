---
title: Bert
author: John Snow Labs
name: bert
date: 2020-07-06 11:33:00 +0800
categories: [Open Source, Models]
tags: [embeddings, transformer, deep learning]
---

#### Model name: *bert*
#### Type: *embeddings*
#### Size: *13.4Mb* 
#### Compatibility: *Spark NLP 2.5.0*
#### License: *Open Source*
#### Inputs: *[document, sentence, token]*
#### Outputs: *[embeddings]*
#### Languages: *[en, multi]*

## Description
BERT embeddings ((Bidirectional Encoder Representations from Transformers))
BERT is a method of pretraining language representations that was used to create models that NLP practicioners can then download and use for free. You can either use these models to extract high quality language features from your text data, or you can fine-tune these models on a specific task (classification, entity recognition, question answering, etc.) with your own data to produce state of the art predictions.
#### Documentation
Relevant documentation (SEO purpose)
#### References
Relevant related work (SEO purpose)
#### Citations 
Relevant citations (SEO purpose)

## Usage example

```python
bert = BertEmbeddings.pretrained(name='bert_base_cased', lang='en')
```
## Run complete example in colab 
Checkout the complete example[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JohnSnowLabs/spark-nlp-workshop/blob/master/tutorials/Certification_Trainings/Public/3.SparkNLP_Pretrained_Models.ipynb)

## See it in action
Not available. 

## Dataset used for training 
ToDo


## Dependencies (components) 
None 


## Evaluation results

ToDo

## Download address
<https://s3.amazonaws.com/auxdata.johnsnowlabs.com/public/models/explain_document_sm_nl_2.5.0_2.4_1588546621618.zip>


